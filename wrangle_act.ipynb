{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle and Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The purpose of this project is to wrangle and analyze data from WeRateDogs, a humorous Twitter account that rates dog photos. I will gather data from various sources, in several formats, assess the quality of the data, clean it, and then provide analysis. \n",
    "\n",
    "From the course **Project Details**, the tasks to accomplish are:\n",
    "- Data wrangling, which consists of:\n",
    " - Gathering data\n",
    " - Assessing data\n",
    " - Cleaning data\n",
    "- Storing, analyzing, and visualizing your wrangled data\n",
    "- Reporting on 1) your data wrangling efforts and 2) your data analyses and visualizations\n",
    "\n",
    "#### Imports\n",
    "Import libraries as dictacted by project description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tweepy\n",
    "import requests as rq\n",
    "import time\n",
    "import json\n",
    "\n",
    "pd.set_option('display.max_colwidth', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "#### Open DogRates Twitter Archive with *Pandas*\n",
    "This file was provided as-is. Open *twitter-archive-enhanced.csv* with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific Dtypes for columns and date columns to parse\n",
    "archive_dtypes = {'tweet_id':str, 'in_reply_to_status_id': str, 'in_reply_to_user_id': str,\n",
    "                  'retweeted_status_id': str, 'retweeted_status_user_id': str, 'rating_numerator': int,\n",
    "                  'rating_denomenator': int}\n",
    "archive_parse_dates = ['timestamp', 'retweeted_status_timestamp']\n",
    "\n",
    "archive_df = pd.read_csv('twitter-archive-enhanced.csv', dtype=archive_dtypes, parse_dates=archive_parse_dates)\n",
    "archive_df.set_index('tweet_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Tweets with *Tweepy*\n",
    "Download the detailed tweet data from Twitter using Tweepy. Save the data to *tweet_json.txt* once downloaded.\n",
    "\n",
    "If the file exists, open it locally to avoid redownloading. (The download takes approximately 30 minutes to complete due to API rate-limiting because extended tweets are not supported by the Tweepy bulk-download function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_count = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # open tweet detailed tweet archive file saved locally\n",
    "        with open('tweet_json.txt', 'r') as fr:\n",
    "            tweets_df = pd.io.json.json_normalize(\n",
    "                        json.load(fr)\n",
    "                     )\n",
    "        break\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        # list of tweets to download from the archive file\n",
    "        tweet_ids = archive_df.tweet_id.unique().tolist()\n",
    "        tweets = []\n",
    "\n",
    "        for t in tweet_ids:\n",
    "            try:\n",
    "                # Parsing Tweepy objects: https://stackoverflow.com/questions/27900451\n",
    "                \n",
    "                twitter_keys = {\"ConsumerAPIKey\":None,\n",
    "                                \"ConsumerSecret\":None,\n",
    "                                \"AccessToken\":None,\n",
    "                                \"AccessTokenSecret\":None}\n",
    "                # comment this out and insert values to twitter_keys above if using other Twitter API keys\n",
    "                twitter_keys = pd.read_json('twitterkeys.json', typ='series')\n",
    "                \n",
    "                twitter_auth = tweepy.OAuthHandler(twitter_keys['ConsumerAPIKey'], twitter_keys['ConsumerSecret'])\n",
    "                twitter_auth.set_access_token(twitter_keys['AccessToken'], twitter_keys['AccessTokenSecret'])\n",
    "                twitter = tweepy.API(twitter_auth, parser=tweepy.parsers.JSONParser(),\n",
    "                                     wait_on_rate_limit=True,\n",
    "                                     wait_on_rate_limit_notify=True)\n",
    "\n",
    "                # get tweets one-by-one because tweepy.statuses_lookup doesn't support tweet_mode=extended\n",
    "                tweets.append(\n",
    "                    twitter.get_status(t, tweet_mode='extended', include_entities=True, trim_user=True)\n",
    "                )\n",
    "\n",
    "            except tweepy.TweepError as te:\n",
    "                #swallow 'status not found' errors, raise all others\n",
    "                if(te.api_code != 144):\n",
    "                    raise\n",
    "\n",
    "            #delay to reduce request rate; 15m limiting causes proxy timeout on intranet    \n",
    "            time.sleep(.2)\n",
    "\n",
    "        # write file to disk to avoid re-downloading\n",
    "        with open('tweet_json.txt', 'w') as fw:\n",
    "            json.dump(tweets, fw)\n",
    "        \n",
    "        # if met maximum retries, raise an error\n",
    "        retry_count = retry_count + 1\n",
    "        if retry_count == 4:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Image Predictions with *Requests*\n",
    "\n",
    "Download the image predictions file using the Requests library. Save the file locally as *image-predictions.tsv*. If the file was already downloaded, open it locally to avoid hitting the server for the same file many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_count = 0\n",
    "\n",
    "while True:\n",
    "    try: \n",
    "        # Open image prediction file saved locally\n",
    "        images_df = pd.read_csv('image-predictions.tsv', sep='\\t', dtype={'tweet_id':str}).set_index('tweet_id')\n",
    "        break\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        # Use Requests library to download\n",
    "        image_predictions_url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "\n",
    "        # Save the file\n",
    "        with open('image-predictions.tsv', 'w') as fr:\n",
    "            r = rq.get(image_predictions_url).text\n",
    "            fr.write(r)\n",
    "        \n",
    "        # if met maximum retries, raise an error\n",
    "        retry_count = retry_count + 1\n",
    "        if retry_count == 4:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns of Archive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "archive_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(archive_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several columns contain data not necessary for this analysis. I intend to remove them for clarity below. Slated for removal:\n",
    "- in_reply_to_status_id\n",
    "- in_reply_to_user_id\n",
    "- source\n",
    "- retweeted_status_user_id\n",
    "- retweeted_status_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rating Denominators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "archive_df.rating_denominator.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rating are almost exclusively out of 10. Ratings out of a multiple of 10 tend to contain multiple dogs. Let's look in to the outlier rating denominators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_denom = [0, 2, 7, 11, 15, 16]\n",
    "archive_df[archive_df.rating_numerator.isin(c_denom)][['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the above ratings appear to be incorrect and will be corrected in the cleaning section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rating Numerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "archive_df.rating_numerator.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more possible numerators, but a few stand out as outliers or non-conforming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_numer = [0, 420, 666, 1776]\n",
    "archive_df[archive_df.rating_numerator.isin(c_numer)][['text', 'rating_numerator', 'rating_denominator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since joke ratings seem valid for a humor account, the tweets that feature dogs will be kept for now. The Snoop Dogg tweet and tweets without dogs will be deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "archive_df.name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large set of tweets have dogs named 'None'. Let's see if that's correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "archive_df[archive_df.name == 'None'].sample(20, random_state=1210)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random sample indicates that these tweets do, in fact, lack names. The first value counts also indicate that when the name is entirely lower case, it tends to be a standard word that was interpreted as a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "archive_df[archive_df.name.str.islower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names that are lower case appear to typically belong to *descriptions* of the dog. Those, and names that are None, will be converted to blanks in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns of Tweets Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tweets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 158 columns in the normalized download of tweets. I'll remove all except the useful columns from this view. For inclusion:\n",
    "- id_str\n",
    "- retweet_count\n",
    "- favorite_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns of Image Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "images_df.reset_index().tweet_id.nunique(), len(images_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All predictions are on single rows, without duplicate tweet_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(1, 3, 1)\n",
    "plt.boxplot(images_df[images_df.p1_dog == True].p1_conf)\n",
    "plt.ylim(ymax=1)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.boxplot(images_df[images_df.p2_dog == True].p2_conf)\n",
    "plt.ylim(ymax=1)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.boxplot(images_df[images_df.p3_dog == True].p3_conf)\n",
    "plt.ylim(ymax=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest confindence predictions come from Prediction 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Tidying Data\n",
    "- Low quality (dirty) data has content issues. Fix 8 of these.\n",
    "- Untidy (messy) data has structural issues. Fix 2 of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Extra Archive Columns\n",
    "Remove columns with data unnecessary for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.drop(columns=['in_reply_to_status_id', 'in_reply_to_user_id',\n",
    "                         'source', 'retweeted_status_user_id',\n",
    "                         'retweeted_status_timestamp'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Extra Tweet Download Columns\n",
    "Remove all columns except those needed for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df[['id_str', 'retweet_count', 'favorite_count']].set_index('id_str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There Aren't Even Dogs in These (Remove Tweets without Dogs)\n",
    "Some tweets had no dogs in them or they weren't specifically about the dog; remove these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_col = ['835152434251116546', '746906459439529985', '670842764863651840', '855862651834028034']\n",
    "archive_df.drop(index=r_col, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Retweets and Tweets with No Photos\n",
    "We love them all, but we want to see original tweets with photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df = archive_df[archive_df.retweeted_status_id.isna() &\n",
    "                        ~archive_df.expanded_urls.isna()]\n",
    "len(archive_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removal of retweets and tweets without photos, 2114 tweets remain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DoggoLingo (Tidy Structure)\n",
    "Combine *doggo, floofer, pupper,* and *puppo* columns in to a single *doggolingo* variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "archive_df[cols] = archive_df[cols].replace('None', '')\n",
    "archive_df['doggolingo'] = archive_df[cols].apply(lambda x: ';'.join(filter(None, x)), axis=1)\n",
    "\n",
    "archive_df.drop(cols, axis='columns', inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Dog Named None (Correct Names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For names that are 'None' or all lower case, convert them to blanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.loc[(archive_df.name=='None') |\n",
    "               (archive_df.name.str.islower()), 'name'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### They're Good Dogs (Correct Ratings)\n",
    "Update ratings that were found with incorrect denominators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ratings = pd.DataFrame([\n",
    "    {'tweet_id': '835246439529840640', 'rating_numerator': 13, 'rating_denominator': 10},\n",
    "    {'tweet_id': '832088576586297345', 'rating_numerator': pd.np.nan, 'rating_denominator': pd.np.nan},\n",
    "    {'tweet_id': '810984652412424192', 'rating_numerator': pd.np.nan, 'rating_denominator': pd.np.nan},\n",
    "    {'tweet_id': '775096608509886464', 'rating_numerator': 14, 'rating_denominator': 10},\n",
    "    {'tweet_id': '740373189193256964', 'rating_numerator': 14, 'rating_denominator': 10},\n",
    "    {'tweet_id': '682962037429899265', 'rating_numerator': 10, 'rating_denominator': 10},\n",
    "    {'tweet_id': '682808988178739200', 'rating_numerator': pd.np.nan, 'rating_denominator': pd.np.nan},\n",
    "    {'tweet_id': '666287406224695296', 'rating_numerator': 9, 'rating_denominator': 10}\n",
    "]).set_index('tweet_id')\n",
    "\n",
    "archive_df.update(update_ratings, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Ratings\n",
    "For ratings that contain multiple dogs (denominators are a multiple of 10), convert the rating to a 10 point scale and save separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df[archive_df.rating_denominator > 10].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Award for the Most Confident Dog (Tidy Structure and Simplify Predictions)\n",
    "Select the prediction that is a dog and is the most confident about dog type as the ultimate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create three temporary DataFrames of info, rename columns to append all stacked\n",
    "images_p1_df = images_df.reset_index()[['tweet_id', 'jpg_url', 'img_num', 'p1', 'p1_conf', 'p1_dog']]\n",
    "images_p2_df = images_df.reset_index()[['tweet_id', 'jpg_url', 'img_num', 'p2', 'p2_conf', 'p2_dog']]\n",
    "images_p3_df = images_df.reset_index()[['tweet_id', 'jpg_url', 'img_num', 'p3', 'p3_conf', 'p3_dog']]\n",
    "\n",
    "images_p1_df.rename(columns={'p1': 'breed', 'p1_conf': 'conf', 'p1_dog': 'is_dog'}, inplace=True)\n",
    "images_p2_df.rename(columns={'p2': 'breed', 'p2_conf': 'conf', 'p2_dog': 'is_dog'}, inplace=True)\n",
    "images_p3_df.rename(columns={'p3': 'breed', 'p3_conf': 'conf', 'p3_dog': 'is_dog'}, inplace=True)\n",
    "\n",
    "a = pd.DataFrame()\n",
    "a = (a.append(images_p1_df)\n",
    "      .append(images_p2_df)\n",
    "      .append(images_p3_df)\n",
    "    )\n",
    "\n",
    "# Select only 'dog' images, then keep only the highest confidence dog image\n",
    "# reset index and massage DataFrame for later use\n",
    "images_df = (a[a.is_dog==True]\n",
    "              .groupby(['tweet_id', 'jpg_url', 'img_num'])\n",
    "              .apply(lambda x: x[x.conf==x.conf.max()])\n",
    "              .drop(columns=['tweet_id', 'jpg_url', 'img_num'])\n",
    "              .reset_index()\n",
    "              .set_index('tweet_id')\n",
    "              .drop(columns=['level_3'])\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Dog Breeds\n",
    "Clean the names of breeds to remove underscores and convert to title case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df.breed = images_df.breed.str.replace('_', ' ').str.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join Tables\n",
    "Join the information from the tweet details downloaded from Twitter and image predictions to the WeRateDogs Twitter archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = archive_df.join(tweets_df)\n",
    "master_df = master_df.join(images_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Twitter Archive Master\n",
    "Store the final cleaned archive according to project requirements.\n",
    "\n",
    "*Store the clean DataFrame(s) in a CSV file with the main one named twitter_archive_master.csv. If additional files exist because multiple tables are required for tidiness, name these files appropriately.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.to_csv('twitter_archive_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
